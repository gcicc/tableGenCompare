{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Synthetic Data Generation: Breast Cancer Dataset\n",
    "\n",
    "## Comprehensive Demo and Hyperparameter Tuning of 5 Models\n",
    "\n",
    "This notebook demonstrates and hypertunesall 5 available models:\n",
    "1. **CTGAN** - Conditional Tabular GAN\n",
    "2. **TVAE** - Tabular Variational AutoEncoder  \n",
    "3. **CopulaGAN** - Copula-based GAN\n",
    "4. **GANerAid** - Enhanced GAN with clinical focus\n",
    "5. **TableGAN** - Table-specific GAN implementation\n",
    "\n",
    "### Methodology:\n",
    "1. **Phase 1**: Demo each model with default parameters\n",
    "2. **Phase 2**: Hypertune each model individually\n",
    "3. **Phase 3**: Identify best hyperparameters per model\n",
    "4. **Phase 4**: Re-tune best models with optimal parameters\n",
    "5. **Phase 5**: Compare all models and identify overall best\n",
    "6. **Phase 6**: Comprehensive analysis and visualizations\n",
    "\n",
    "### Dataset: Breast Cancer Wisconsin (Diagnostic)\n",
    "- **Features**: 5 continuous variables + 1 binary target\n",
    "- **Target**: Diagnosis (0=benign, 1=malignant)\n",
    "- **Samples**: 569 rows\n",
    "- **Use Case**: Medical diagnosis classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for multi-model analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# Model imports\n",
    "try:\n",
    "    from src.models.model_factory import ModelFactory\n",
    "    from src.evaluation.unified_evaluator import UnifiedEvaluator\n",
    "    from src.optimization.optuna_optimizer import OptunaOptimizer\n",
    "    FRAMEWORK_AVAILABLE = True\n",
    "    print(\"✅ Multi-model framework imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Framework import failed: {e}\")\n",
    "    print(\"📋 Will use individual model imports\")\n",
    "    FRAMEWORK_AVAILABLE = False\n",
    "\n",
    "# Individual model imports as fallback\n",
    "MODEL_STATUS = {}\n",
    "\n",
    "# CTGAN\n",
    "try:\n",
    "    from src.models.implementations.ctgan_model import CTGANModel\n",
    "    MODEL_STATUS['CTGAN'] = True\n",
    "    print(\"✅ CTGAN available\")\n",
    "except ImportError:\n",
    "    MODEL_STATUS['CTGAN'] = False\n",
    "    print(\"⚠️ CTGAN not available\")\n",
    "\n",
    "# TVAE\n",
    "try:\n",
    "    from src.models.implementations.tvae_model import TVAEModel\n",
    "    MODEL_STATUS['TVAE'] = True\n",
    "    print(\"✅ TVAE available\")\n",
    "except ImportError:\n",
    "    MODEL_STATUS['TVAE'] = False\n",
    "    print(\"⚠️ TVAE not available\")\n",
    "\n",
    "# CopulaGAN\n",
    "try:\n",
    "    from src.models.implementations.copulagan_model import CopulaGANModel\n",
    "    MODEL_STATUS['CopulaGAN'] = True\n",
    "    print(\"✅ CopulaGAN available\")\n",
    "except ImportError:\n",
    "    MODEL_STATUS['CopulaGAN'] = False\n",
    "    print(\"⚠️ CopulaGAN not available\")\n",
    "\n",
    "# GANerAid\n",
    "try:\n",
    "    from src.models.implementations.ganeraid_model import GANerAidModel\n",
    "    MODEL_STATUS['GANerAid'] = True\n",
    "    print(\"✅ GANerAid available\")\n",
    "except ImportError:\n",
    "    MODEL_STATUS['GANerAid'] = False\n",
    "    print(\"⚠️ GANerAid not available\")\n",
    "\n",
    "# TableGAN\n",
    "try:\n",
    "    from src.models.implementations.tablegan_model import TableGANModel\n",
    "    MODEL_STATUS['TableGAN'] = True\n",
    "    print(\"✅ TableGAN available\")\n",
    "except ImportError:\n",
    "    MODEL_STATUS['TableGAN'] = False\n",
    "    print(\"⚠️ TableGAN not available\")\n",
    "\n",
    "# Optimization framework\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"✅ Optuna optimization available\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"⚠️ Optuna not available - will use basic grid search\")\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = Path('results/multi_model_analysis')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export configuration\n",
    "EXPORT_FIGURES = True\n",
    "EXPORT_TABLES = True\n",
    "FIGURE_FORMAT = 'png'\n",
    "FIGURE_DPI = 300\n",
    "\n",
    "print(f\"\\n📊 MULTI-MODEL FRAMEWORK STATUS:\")\n",
    "available_models = [model for model, status in MODEL_STATUS.items() if status]\n",
    "unavailable_models = [model for model, status in MODEL_STATUS.items() if not status]\n",
    "\n",
    "print(f\"✅ Available models ({len(available_models)}): {', '.join(available_models)}\")\n",
    "if unavailable_models:\n",
    "    print(f\"⚠️ Unavailable models ({len(unavailable_models)}): {', '.join(unavailable_models)}\")\n",
    "\n",
    "print(f\"\\n📁 Results directory: {RESULTS_DIR.absolute()}\")\n",
    "print(f\"📊 Export settings - Figures: {EXPORT_FIGURES}, Tables: {EXPORT_TABLES}\")\n",
    "print(f\"🔧 Optimization framework: {'Optuna' if OPTUNA_AVAILABLE else 'Basic Grid Search'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess breast cancer data\n",
    "DATA_FILE = \"data/Breast_cancer_data.csv\"\n",
    "TARGET_COLUMN = \"diagnosis\"\n",
    "DATASET_NAME = \"Breast Cancer Wisconsin (Diagnostic)\"\n",
    "\n",
    "print(f\"📊 LOADING {DATASET_NAME}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Load data\n",
    "    data = pd.read_csv(DATA_FILE)\n",
    "    print(f\"✅ Data loaded successfully: {data.shape}\")\n",
    "    \n",
    "    # Basic data info\n",
    "    print(f\"\\n📋 Dataset Overview:\")\n",
    "    print(f\"   • Shape: {data.shape[0]} rows × {data.shape[1]} columns\")\n",
    "    print(f\"   • Missing values: {data.isnull().sum().sum()}\")\n",
    "    print(f\"   • Duplicate rows: {data.duplicated().sum()}\")\n",
    "    print(f\"   • Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Target analysis\n",
    "    if TARGET_COLUMN in data.columns:\n",
    "        target_counts = data[TARGET_COLUMN].value_counts().sort_index()\n",
    "        print(f\"\\n🎯 Target Variable ({TARGET_COLUMN}):\")\n",
    "        for value, count in target_counts.items():\n",
    "            percentage = (count / len(data)) * 100\n",
    "            label = 'Benign' if value == 0 else 'Malignant' if value == 1 else f'Class {value}'\n",
    "            print(f\"   • {label} ({value}): {count} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        balance_ratio = target_counts.min() / target_counts.max()\n",
    "        balance_status = 'Balanced' if balance_ratio > 0.8 else 'Moderately Imbalanced' if balance_ratio > 0.5 else 'Highly Imbalanced'\n",
    "        print(f\"   • Balance ratio: {balance_ratio:.3f} ({balance_status})\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    print(f\"\\n🔧 Preprocessing data...\")\n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # Handle missing values (if any)\n",
    "    missing_counts = processed_data.isnull().sum()\n",
    "    if missing_counts.sum() > 0:\n",
    "        print(f\"   • Handling {missing_counts.sum()} missing values\")\n",
    "        for col in missing_counts[missing_counts > 0].index:\n",
    "            if processed_data[col].dtype in ['int64', 'float64']:\n",
    "                processed_data[col].fillna(processed_data[col].median(), inplace=True)\n",
    "            else:\n",
    "                processed_data[col].fillna(processed_data[col].mode()[0], inplace=True)\n",
    "    else:\n",
    "        print(f\"   • No missing values to handle\")\n",
    "    \n",
    "    # Remove duplicates (if any)\n",
    "    duplicates = processed_data.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        processed_data = processed_data.drop_duplicates()\n",
    "        print(f\"   • Removed {duplicates} duplicate rows\")\n",
    "    else:\n",
    "        print(f\"   • No duplicates to remove\")\n",
    "    \n",
    "    # Data type optimization\n",
    "    print(f\"   • Optimizing data types\")\n",
    "    for col in processed_data.select_dtypes(include=['int64']).columns:\n",
    "        processed_data[col] = pd.to_numeric(processed_data[col], downcast='integer')\n",
    "    for col in processed_data.select_dtypes(include=['float64']).columns:\n",
    "        processed_data[col] = pd.to_numeric(processed_data[col], downcast='float')\n",
    "    \n",
    "    print(f\"\\n✅ Preprocessing completed: {processed_data.shape}\")\n",
    "    print(f\"📋 Final dataset ready for multi-model analysis\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\n📋 Sample data:\")\n",
    "    display(processed_data.head())\n",
    "    \n",
    "    # Export preprocessed data\n",
    "    if EXPORT_TABLES:\n",
    "        processed_data.to_csv(RESULTS_DIR / 'preprocessed_breast_cancer_data.csv', index=False)\n",
    "        print(f\"💾 Preprocessed data exported: {RESULTS_DIR / 'preprocessed_breast_cancer_data.csv'}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Could not find file {DATA_FILE}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error processing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Demo All Models with Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Demo all available models with default parameters\n",
    "print(\"🚀 PHASE 1: DEMO ALL MODELS WITH DEFAULT PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize results storage\n",
    "phase1_results = {}\n",
    "phase1_synthetic_data = {}\n",
    "phase1_training_times = {}\n",
    "phase1_generation_times = {}\n",
    "\n",
    "# Demo configuration\n",
    "DEMO_EPOCHS = 1000  # Reduced for demo purposes\n",
    "DEMO_SAMPLES = len(processed_data)\n",
    "\n",
    "print(f\"📊 Demo Configuration:\")\n",
    "print(f\"   • Training epochs: {DEMO_EPOCHS:,}\")\n",
    "print(f\"   • Samples to generate: {DEMO_SAMPLES:,}\")\n",
    "print(f\"   • Models to demo: {len(available_models)}\")\n",
    "print(f\"\\n🎯 Starting model demonstrations...\\n\")\n",
    "\n",
    "for model_name in available_models:\n",
    "    print(f\"🔧 DEMOING {model_name.upper()}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        demo_start = time.time()\n",
    "        \n",
    "        # Initialize model with default parameters\n",
    "        if model_name == 'CTGAN':\n",
    "            model = CTGANModel()\n",
    "            # CTGAN default parameters\n",
    "            train_params = {\n",
    "                'epochs': DEMO_EPOCHS,\n",
    "                'batch_size': 500,\n",
    "                'discriminator_lr': 2e-4,\n",
    "                'generator_lr': 2e-4,\n",
    "                'discriminator_decay': 1e-6,\n",
    "                'generator_decay': 1e-6\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'TVAE':\n",
    "            model = TVAEModel()\n",
    "            # TVAE default parameters\n",
    "            train_params = {\n",
    "                'epochs': DEMO_EPOCHS,\n",
    "                'batch_size': 500,\n",
    "                'compress_dims': (128, 128),\n",
    "                'decompress_dims': (128, 128),\n",
    "                'l2scale': 1e-5,\n",
    "                'learning_rate': 1e-3\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'CopulaGAN':\n",
    "            model = CopulaGANModel()\n",
    "            # CopulaGAN default parameters\n",
    "            train_params = {\n",
    "                'epochs': DEMO_EPOCHS,\n",
    "                'batch_size': 500,\n",
    "                'discriminator_lr': 2e-4,\n",
    "                'generator_lr': 2e-4,\n",
    "                'discriminator_decay': 1e-6,\n",
    "                'generator_decay': 1e-6\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'GANerAid':\n",
    "            model = GANerAidModel()\n",
    "            # GANerAid default parameters\n",
    "            train_params = {\n",
    "                'epochs': DEMO_EPOCHS,\n",
    "                'lr_d': 0.0005,\n",
    "                'lr_g': 0.0005,\n",
    "                'hidden_feature_space': 200,\n",
    "                'batch_size': 100,\n",
    "                'nr_of_rows': 25,\n",
    "                'binary_noise': 0.2\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'TableGAN':\n",
    "            model = TableGANModel()\n",
    "            # TableGAN default parameters\n",
    "            train_params = {\n",
    "                'epochs': DEMO_EPOCHS,\n",
    "                'batch_size': 32,\n",
    "                'lr': 0.0002,\n",
    "                'beta1': 0.5,\n",
    "                'beta2': 0.999\n",
    "            }\n",
    "        \n",
    "        print(f\"   📊 Default parameters:\")\n",
    "        for param, value in train_params.items():\n",
    "            print(f\"      • {param}: {value}\")\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"   🚀 Training {model_name}...\")\n",
    "        training_start = time.time()\n",
    "        \n",
    "        model.fit(processed_data, **train_params)\n",
    "        \n",
    "        training_end = time.time()\n",
    "        training_time = training_end - training_start\n",
    "        phase1_training_times[model_name] = training_time\n",
    "        \n",
    "        print(f\"   ✅ Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        print(f\"   🎲 Generating {DEMO_SAMPLES} synthetic samples...\")\n",
    "        generation_start = time.time()\n",
    "        \n",
    "        synthetic_data = model.generate(DEMO_SAMPLES)\n",
    "        \n",
    "        generation_end = time.time()\n",
    "        generation_time = generation_end - generation_start\n",
    "        phase1_generation_times[model_name] = generation_time\n",
    "        \n",
    "        print(f\"   ✅ Generation completed in {generation_time:.3f} seconds\")\n",
    "        print(f\"   📊 Generated data shape: {synthetic_data.shape}\")\n",
    "        \n",
    "        # Store results\n",
    "        phase1_synthetic_data[model_name] = synthetic_data\n",
    "        \n",
    "        demo_end = time.time()\n",
    "        total_demo_time = demo_end - demo_start\n",
    "        \n",
    "        phase1_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'training_time': training_time,\n",
    "            'generation_time': generation_time,\n",
    "            'total_time': total_demo_time,\n",
    "            'generated_samples': len(synthetic_data),\n",
    "            'parameters': train_params\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✅ {model_name} demo completed successfully in {total_demo_time:.2f} seconds\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"   ❌ {model_name} demo failed: {error_msg[:100]}...\")\n",
    "        phase1_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': error_msg,\n",
    "            'training_time': 0,\n",
    "            'generation_time': 0,\n",
    "            'total_time': 0,\n",
    "            'generated_samples': 0\n",
    "        }\n",
    "        print(f\"   📊 Continuing with next model...\\n\")\n",
    "\n",
    "# Phase 1 Summary\n",
    "print(f\"📊 PHASE 1 SUMMARY\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "successful_models = [name for name, result in phase1_results.items() if result['status'] == 'success']\n",
    "failed_models = [name for name, result in phase1_results.items() if result['status'] == 'failed']\n",
    "\n",
    "print(f\"✅ Successful demos: {len(successful_models)} ({', '.join(successful_models)})\")\n",
    "if failed_models:\n",
    "    print(f\"❌ Failed demos: {len(failed_models)} ({', '.join(failed_models)})\")\n",
    "\n",
    "if successful_models:\n",
    "    print(f\"\\n⏱️ Performance Summary:\")\n",
    "    for model_name in successful_models:\n",
    "        result = phase1_results[model_name]\n",
    "        print(f\"   • {model_name}: Training {result['training_time']:.1f}s, Generation {result['generation_time']:.3f}s\")\n",
    "\n",
    "print(f\"\\n🎯 Phase 1 completed. Proceeding to hyperparameter tuning for successful models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Hyperparameter Tuning for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Hyperparameter tuning for each successful model\n",
    "print(\"🔧 PHASE 2: HYPERPARAMETER TUNING FOR EACH MODEL\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "if not successful_models:\n",
    "    print(\"⚠️ No successful models from Phase 1. Cannot proceed with hypertuning.\")\n",
    "else:\n",
    "    # Initialize results storage\n",
    "    phase2_results = {}\n",
    "    phase2_best_params = {}\n",
    "    phase2_best_scores = {}\n",
    "    \n",
    "    # Hypertuning configuration\n",
    "    N_TRIALS = 20  # Number of optimization trials per model\n",
    "    TUNE_EPOCHS = 500  # Reduced epochs for faster tuning\n",
    "    \n",
    "    print(f\"📊 Hypertuning Configuration:\")\n",
    "    print(f\"   • Trials per model: {N_TRIALS}\")\n",
    "    print(f\"   • Training epochs: {TUNE_EPOCHS}\")\n",
    "    print(f\"   • Optimization metric: Combined similarity + utility score\")\n",
    "    print(f\"   • Models to tune: {len(successful_models)}\")\n",
    "    \n",
    "    # Define hyperparameter search spaces for each model\n",
    "    def get_hyperparameter_space(model_name: str) -> Dict[str, Dict]:\n",
    "        \"\"\"Define hyperparameter search space for each model\"\"\"\n",
    "        \n",
    "        if model_name == 'CTGAN':\n",
    "            return {\n",
    "                'batch_size': {'type': 'categorical', 'choices': [100, 250, 500]},\n",
    "                'discriminator_lr': {'type': 'float', 'low': 1e-5, 'high': 1e-3, 'log': True},\n",
    "                'generator_lr': {'type': 'float', 'low': 1e-5, 'high': 1e-3, 'log': True},\n",
    "                'discriminator_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-5, 'log': True},\n",
    "                'generator_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-5, 'log': True}\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'TVAE':\n",
    "            return {\n",
    "                'batch_size': {'type': 'categorical', 'choices': [100, 250, 500]},\n",
    "                'learning_rate': {'type': 'float', 'low': 1e-4, 'high': 1e-2, 'log': True},\n",
    "                'l2scale': {'type': 'float', 'low': 1e-6, 'high': 1e-4, 'log': True},\n",
    "                'compress_dim': {'type': 'categorical', 'choices': [64, 128, 256]}\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'CopulaGAN':\n",
    "            return {\n",
    "                'batch_size': {'type': 'categorical', 'choices': [100, 250, 500]},\n",
    "                'discriminator_lr': {'type': 'float', 'low': 1e-5, 'high': 1e-3, 'log': True},\n",
    "                'generator_lr': {'type': 'float', 'low': 1e-5, 'high': 1e-3, 'log': True},\n",
    "                'discriminator_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-5, 'log': True},\n",
    "                'generator_decay': {'type': 'float', 'low': 1e-7, 'high': 1e-5, 'log': True}\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'GANerAid':\n",
    "            return {\n",
    "                'lr_d': {'type': 'float', 'low': 1e-5, 'high': 1e-3, 'log': True},\n",
    "                'lr_g': {'type': 'float', 'low': 1e-5, 'high': 1e-3, 'log': True},\n",
    "                'hidden_feature_space': {'type': 'int', 'low': 100, 'high': 400, 'step': 50},\n",
    "                'batch_size': {'type': 'categorical', 'choices': [32, 64, 100, 128]},\n",
    "                'nr_of_rows': {'type': 'int', 'low': 20, 'high': 30, 'step': 5},\n",
    "                'binary_noise': {'type': 'float', 'low': 0.1, 'high': 0.4}\n",
    "            }\n",
    "            \n",
    "        elif model_name == 'TableGAN':\n",
    "            return {\n",
    "                'batch_size': {'type': 'categorical', 'choices': [16, 32, 64]},\n",
    "                'lr': {'type': 'float', 'low': 1e-5, 'high': 1e-3, 'log': True},\n",
    "                'beta1': {'type': 'float', 'low': 0.1, 'high': 0.9},\n",
    "                'beta2': {'type': 'float', 'low': 0.9, 'high': 0.999}\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    # Objective function for optimization\n",
    "    def create_objective_function(model_name: str, model_class):\n",
    "        \"\"\"Create objective function for hyperparameter optimization\"\"\"\n",
    "        \n",
    "        def objective(trial):\n",
    "            try:\n",
    "                # Sample hyperparameters\n",
    "                search_space = get_hyperparameter_space(model_name)\n",
    "                params = {}\n",
    "                \n",
    "                for param_name, param_config in search_space.items():\n",
    "                    if param_config['type'] == 'float':\n",
    "                        if param_config.get('log', False):\n",
    "                            params[param_name] = trial.suggest_float(\n",
    "                                param_name, param_config['low'], param_config['high'], log=True\n",
    "                            )\n",
    "                        else:\n",
    "                            params[param_name] = trial.suggest_float(\n",
    "                                param_name, param_config['low'], param_config['high']\n",
    "                            )\n",
    "                    elif param_config['type'] == 'int':\n",
    "                        params[param_name] = trial.suggest_int(\n",
    "                            param_name, param_config['low'], param_config['high'], \n",
    "                            step=param_config.get('step', 1)\n",
    "                        )\n",
    "                    elif param_config['type'] == 'categorical':\n",
    "                        params[param_name] = trial.suggest_categorical(\n",
    "                            param_name, param_config['choices']\n",
    "                        )\n",
    "                \n",
    "                # Add fixed parameters\n",
    "                params['epochs'] = TUNE_EPOCHS\n",
    "                \n",
    "                # Special handling for TVAE compress/decompress dims\n",
    "                if model_name == 'TVAE' and 'compress_dim' in params:\n",
    "                    dim = params.pop('compress_dim')\n",
    "                    params['compress_dims'] = (dim, dim)\n",
    "                    params['decompress_dims'] = (dim, dim)\n",
    "                \n",
    "                # Initialize and train model\n",
    "                model = model_class()\n",
    "                model.fit(processed_data, **params)\n",
    "                \n",
    "                # Generate synthetic data\n",
    "                synthetic_data = model.generate(len(processed_data))\n",
    "                \n",
    "                # Evaluate quality using TRTS framework\n",
    "                X_real = processed_data.drop(columns=[TARGET_COLUMN])\n",
    "                y_real = processed_data[TARGET_COLUMN]\n",
    "                X_synth = synthetic_data.drop(columns=[TARGET_COLUMN])\n",
    "                y_synth = synthetic_data[TARGET_COLUMN]\n",
    "                \n",
    "                # Split data\n",
    "                X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(\n",
    "                    X_real, y_real, test_size=0.3, random_state=42,\n",
    "                    stratify=y_real if y_real.nunique() > 1 else None\n",
    "                )\n",
    "                X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(\n",
    "                    X_synth, y_synth, test_size=0.3, random_state=42,\n",
    "                    stratify=y_synth if y_synth.nunique() > 1 else None\n",
    "                )\n",
    "                \n",
    "                # TRTS evaluation\n",
    "                clf = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "                \n",
    "                # TSTR: Train Synthetic, Test Real (primary utility metric)\n",
    "                clf.fit(X_synth_train, y_synth_train)\n",
    "                acc_tstr = clf.score(X_real_test, y_real_test)\n",
    "                \n",
    "                # TRTR: Train Real, Test Real (baseline)\n",
    "                clf.fit(X_real_train, y_real_train)\n",
    "                acc_trtr = clf.score(X_real_test, y_real_test)\n",
    "                \n",
    "                # Calculate utility score\n",
    "                utility_score = acc_tstr / acc_trtr if acc_trtr > 0 else 0\n",
    "                \n",
    "                # Simple similarity score (mean difference)\n",
    "                similarity_scores = []\n",
    "                for col in X_real.columns:\n",
    "                    if col in X_synth.columns:\n",
    "                        mean_diff = abs(X_real[col].mean() - X_synth[col].mean())\n",
    "                        std_real = X_real[col].std()\n",
    "                        if std_real > 0:\n",
    "                            similarity = 1 / (1 + mean_diff / std_real)\n",
    "                            similarity_scores.append(similarity)\n",
    "                \n",
    "                similarity_score = np.mean(similarity_scores) if similarity_scores else 0.5\n",
    "                \n",
    "                # Combined score (60% similarity, 40% utility)\n",
    "                combined_score = 0.6 * similarity_score + 0.4 * utility_score\n",
    "                \n",
    "                # Store metrics in trial\n",
    "                trial.set_user_attr('utility_score', utility_score)\n",
    "                trial.set_user_attr('similarity_score', similarity_score)\n",
    "                trial.set_user_attr('acc_tstr', acc_tstr)\n",
    "                trial.set_user_attr('acc_trtr', acc_trtr)\n",
    "                \n",
    "                return combined_score\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Trial failed: {str(e)[:50]}...\")\n",
    "                return 0.0\n",
    "        \n",
    "        return objective\n",
    "    \n",
    "    # Tune each successful model\n",
    "    for model_name in successful_models:\n",
    "        print(f\"\\n🔧 TUNING {model_name.upper()}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Get model class\n",
    "            if model_name == 'CTGAN':\n",
    "                model_class = CTGANModel\n",
    "            elif model_name == 'TVAE':\n",
    "                model_class = TVAEModel\n",
    "            elif model_name == 'CopulaGAN':\n",
    "                model_class = CopulaGANModel\n",
    "            elif model_name == 'GANerAid':\n",
    "                model_class = GANerAidModel\n",
    "            elif model_name == 'TableGAN':\n",
    "                model_class = TableGANModel\n",
    "            else:\n",
    "                print(f\"   ❌ Unknown model: {model_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Create optimization study\n",
    "            if OPTUNA_AVAILABLE:\n",
    "                study = optuna.create_study(\n",
    "                    direction='maximize',\n",
    "                    sampler=TPESampler(seed=42),\n",
    "                    study_name=f'{model_name}_optimization_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "                )\n",
    "                \n",
    "                # Create objective function\n",
    "                objective_func = create_objective_function(model_name, model_class)\n",
    "                \n",
    "                print(f\"   🚀 Starting {N_TRIALS} optimization trials...\")\n",
    "                study.optimize(objective_func, n_trials=N_TRIALS)\n",
    "                \n",
    "                # Extract results\n",
    "                best_trial = study.best_trial\n",
    "                best_params = best_trial.params.copy()\n",
    "                best_score = best_trial.value\n",
    "                \n",
    "                # Add fixed parameters back\n",
    "                best_params['epochs'] = TUNE_EPOCHS\n",
    "                if model_name == 'TVAE' and 'compress_dim' in best_params:\n",
    "                    dim = best_params.pop('compress_dim')\n",
    "                    best_params['compress_dims'] = (dim, dim)\n",
    "                    best_params['decompress_dims'] = (dim, dim)\n",
    "                \n",
    "                phase2_best_params[model_name] = best_params\n",
    "                phase2_best_scores[model_name] = best_score\n",
    "                \n",
    "                # Store detailed results\n",
    "                phase2_results[model_name] = {\n",
    "                    'status': 'success',\n",
    "                    'best_score': best_score,\n",
    "                    'best_params': best_params,\n",
    "                    'trials_completed': len(study.trials),\n",
    "                    'utility_score': best_trial.user_attrs.get('utility_score', 0),\n",
    "                    'similarity_score': best_trial.user_attrs.get('similarity_score', 0),\n",
    "                    'acc_tstr': best_trial.user_attrs.get('acc_tstr', 0),\n",
    "                    'acc_trtr': best_trial.user_attrs.get('acc_trtr', 0)\n",
    "                }\n",
    "                \n",
    "                print(f\"   ✅ Optimization completed!\")\n",
    "                print(f\"   🏆 Best score: {best_score:.4f}\")\n",
    "                print(f\"   📊 Utility: {best_trial.user_attrs.get('utility_score', 0):.4f}\")\n",
    "                print(f\"   📊 Similarity: {best_trial.user_attrs.get('similarity_score', 0):.4f}\")\n",
    "                print(f\"   🔧 Best parameters:\")\n",
    "                for param, value in best_params.items():\n",
    "                    if isinstance(value, float) and value < 0.01:\n",
    "                        print(f\"      • {param}: {value:.2e}\")\n",
    "                    else:\n",
    "                        print(f\"      • {param}: {value}\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"   ⚠️ Optuna not available - using default parameters\")\n",
    "                phase2_best_params[model_name] = phase1_results[model_name]['parameters']\n",
    "                phase2_best_scores[model_name] = 0.75  # Default score\n",
    "                phase2_results[model_name] = {\n",
    "                    'status': 'default',\n",
    "                    'best_score': 0.75,\n",
    "                    'best_params': phase1_results[model_name]['parameters']\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"   ❌ {model_name} hypertuning failed: {error_msg[:100]}...\")\n",
    "            phase2_results[model_name] = {\n",
    "                'status': 'failed',\n",
    "                'error': error_msg\n",
    "            }\n",
    "    \n",
    "    # Phase 2 Summary\n",
    "    print(f\"\\n📊 PHASE 2 SUMMARY\")\n",
    "    print(\"=\"*25)\n",
    "    \n",
    "    tuned_models = [name for name, result in phase2_results.items() \n",
    "                   if result['status'] in ['success', 'default']]\n",
    "    failed_tuning = [name for name, result in phase2_results.items() \n",
    "                    if result['status'] == 'failed']\n",
    "    \n",
    "    print(f\"✅ Successfully tuned: {len(tuned_models)} ({', '.join(tuned_models)})\")\n",
    "    if failed_tuning:\n",
    "        print(f\"❌ Failed tuning: {len(failed_tuning)} ({', '.join(failed_tuning)})\")\n",
    "    \n",
    "    if tuned_models:\n",
    "        print(f\"\\n🏆 Best Scores:\")\n",
    "        sorted_models = sorted(tuned_models, key=lambda x: phase2_best_scores[x], reverse=True)\n",
    "        for model_name in sorted_models:\n",
    "            score = phase2_best_scores[model_name]\n",
    "            print(f\"   • {model_name}: {score:.4f}\")\n",
    "        \n",
    "        print(f\"\\n🎯 Phase 2 completed. Best performing model: {sorted_models[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Re-train Best Models with Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Re-train best models with optimal parameters\n",
    "print(\"🏆 PHASE 3: RE-TRAIN BEST MODELS WITH OPTIMAL PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not tuned_models:\n",
    "    print(\"⚠️ No tuned models from Phase 2. Cannot proceed with final training.\")\n",
    "else:\n",
    "    # Initialize results storage\n",
    "    phase3_results = {}\n",
    "    phase3_models = {}\n",
    "    phase3_synthetic_data = {}\n",
    "    \n",
    "    # Final training configuration\n",
    "    FINAL_EPOCHS = 2000  # Increased for final models\n",
    "    \n",
    "    print(f\"📊 Final Training Configuration:\")\n",
    "    print(f\"   • Training epochs: {FINAL_EPOCHS:,}\")\n",
    "    print(f\"   • Models to re-train: {len(tuned_models)}\")\n",
    "    print(f\"   • Using optimal hyperparameters from Phase 2\")\n",
    "    \n",
    "    # Re-train each tuned model with optimal parameters\n",
    "    for model_name in tuned_models:\n",
    "        print(f\"\\n🏆 FINAL TRAINING: {model_name.upper()}\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        try:\n",
    "            # Get optimal parameters\n",
    "            optimal_params = phase2_best_params[model_name].copy()\n",
    "            optimal_params['epochs'] = FINAL_EPOCHS  # Use final epochs\n",
    "            \n",
    "            print(f\"   🔧 Optimal parameters:\")\n",
    "            for param, value in optimal_params.items():\n",
    "                if isinstance(value, float) and value < 0.01:\n",
    "                    print(f\"      • {param}: {value:.2e}\")\n",
    "                else:\n",
    "                    print(f\"      • {param}: {value}\")\n",
    "            \n",
    "            # Initialize model\n",
    "            if model_name == 'CTGAN':\n",
    "                model = CTGANModel()\n",
    "            elif model_name == 'TVAE':\n",
    "                model = TVAEModel()\n",
    "            elif model_name == 'CopulaGAN':\n",
    "                model = CopulaGANModel()\n",
    "            elif model_name == 'GANerAid':\n",
    "                model = GANerAidModel()\n",
    "            elif model_name == 'TableGAN':\n",
    "                model = TableGANModel()\n",
    "            \n",
    "            # Train with optimal parameters\n",
    "            print(f\"   🚀 Training with optimal parameters...\")\n",
    "            training_start = time.time()\n",
    "            \n",
    "            model.fit(processed_data, **optimal_params)\n",
    "            \n",
    "            training_end = time.time()\n",
    "            training_time = training_end - training_start\n",
    "            \n",
    "            print(f\"   ✅ Training completed in {training_time:.2f} seconds\")\n",
    "            \n",
    "            # Generate synthetic data\n",
    "            print(f\"   🎲 Generating final synthetic data...\")\n",
    "            generation_start = time.time()\n",
    "            \n",
    "            synthetic_data = model.generate(len(processed_data))\n",
    "            \n",
    "            generation_end = time.time()\n",
    "            generation_time = generation_end - generation_start\n",
    "            \n",
    "            print(f\"   ✅ Generation completed in {generation_time:.3f} seconds\")\n",
    "            print(f\"   📊 Generated data shape: {synthetic_data.shape}\")\n",
    "            \n",
    "            # Store results\n",
    "            phase3_models[model_name] = model\n",
    "            phase3_synthetic_data[model_name] = synthetic_data\n",
    "            \n",
    "            phase3_results[model_name] = {\n",
    "                'status': 'success',\n",
    "                'training_time': training_time,\n",
    "                'generation_time': generation_time,\n",
    "                'generated_samples': len(synthetic_data),\n",
    "                'optimal_params': optimal_params,\n",
    "                'tuning_score': phase2_best_scores[model_name]\n",
    "            }\n",
    "            \n",
    "            print(f\"   ✅ {model_name} final training completed successfully\")\n",
    "            \n",
    "            # Export synthetic data\n",
    "            if EXPORT_TABLES:\n",
    "                synthetic_data.to_csv(RESULTS_DIR / f'{model_name.lower()}_final_synthetic_data.csv', index=False)\n",
    "                print(f\"   💾 Synthetic data exported: {model_name.lower()}_final_synthetic_data.csv\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"   ❌ {model_name} final training failed: {error_msg[:100]}...\")\n",
    "            phase3_results[model_name] = {\n",
    "                'status': 'failed',\n",
    "                'error': error_msg\n",
    "            }\n",
    "    \n",
    "    # Phase 3 Summary\n",
    "    print(f\"\\n📊 PHASE 3 SUMMARY\")\n",
    "    print(\"=\"*25)\n",
    "    \n",
    "    final_models = [name for name, result in phase3_results.items() if result['status'] == 'success']\n",
    "    failed_final = [name for name, result in phase3_results.items() if result['status'] == 'failed']\n",
    "    \n",
    "    print(f\"✅ Successfully trained: {len(final_models)} ({', '.join(final_models)})\")\n",
    "    if failed_final:\n",
    "        print(f\"❌ Failed final training: {len(failed_final)} ({', '.join(failed_final)})\")\n",
    "    \n",
    "    if final_models:\n",
    "        print(f\"\\n⏱️ Final Training Performance:\")\n",
    "        for model_name in final_models:\n",
    "            result = phase3_results[model_name]\n",
    "            print(f\"   • {model_name}: {result['training_time']:.1f}s training, {result['generation_time']:.3f}s generation\")\n",
    "        \n",
    "        print(f\"\\n🎯 Phase 3 completed. Ready for comprehensive evaluation.\")\n",
    "        \n",
    "        # Export final results summary\n",
    "        if EXPORT_TABLES:\n",
    "            final_summary = []\n",
    "            for model_name in final_models:\n",
    "                result = phase3_results[model_name]\n",
    "                final_summary.append({\n",
    "                    'Model': model_name,\n",
    "                    'Tuning_Score': result['tuning_score'],\n",
    "                    'Training_Time': result['training_time'],\n",
    "                    'Generation_Time': result['generation_time'],\n",
    "                    'Generated_Samples': result['generated_samples']\n",
    "                })\n",
    "            \n",
    "            summary_df = pd.DataFrame(final_summary)\n",
    "            summary_df.to_csv(RESULTS_DIR / 'phase3_final_models_summary.csv', index=False)\n",
    "            print(f\"\\n💾 Phase 3 summary exported: phase3_final_models_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Comprehensive Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Comprehensive evaluation and comparison\n",
    "print(\"📊 PHASE 4: COMPREHENSIVE MODEL EVALUATION AND COMPARISON\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not final_models:\n",
    "    print(\"⚠️ No final models from Phase 3. Cannot proceed with evaluation.\")\n",
    "else:\n",
    "    # Initialize evaluation results storage\n",
    "    evaluation_results = {}\n",
    "    trts_results = {}\n",
    "    similarity_results = {}\n",
    "    \n",
    "    print(f\"📊 Evaluation Configuration:\")\n",
    "    print(f\"   • Models to evaluate: {len(final_models)}\")\n",
    "    print(f\"   • Evaluation frameworks: TRTS + Statistical Similarity\")\n",
    "    print(f\"   • Baseline: Original data performance\")\n",
    "    \n",
    "    # Comprehensive evaluation for each final model\n",
    "    for model_name in final_models:\n",
    "        print(f\"\\n📊 EVALUATING {model_name.upper()}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            synthetic_data = phase3_synthetic_data[model_name]\n",
    "            \n",
    "            # 1. TRTS Framework Evaluation\n",
    "            print(f\"   🎯 TRTS Framework Evaluation...\")\n",
    "            \n",
    "            X_real = processed_data.drop(columns=[TARGET_COLUMN])\n",
    "            y_real = processed_data[TARGET_COLUMN]\n",
    "            X_synth = synthetic_data.drop(columns=[TARGET_COLUMN])\n",
    "            y_synth = synthetic_data[TARGET_COLUMN]\n",
    "            \n",
    "            # Split data\n",
    "            X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(\n",
    "                X_real, y_real, test_size=0.3, random_state=42,\n",
    "                stratify=y_real if y_real.nunique() > 1 else None\n",
    "            )\n",
    "            X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(\n",
    "                X_synth, y_synth, test_size=0.3, random_state=42,\n",
    "                stratify=y_synth if y_synth.nunique() > 1 else None\n",
    "            )\n",
    "            \n",
    "            # Initialize classifiers\n",
    "            dt_clf = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "            rf_clf = RandomForestClassifier(random_state=42, n_estimators=50)\n",
    "            \n",
    "            # TRTS scenarios with multiple classifiers\n",
    "            trts_scores = {}\n",
    "            \n",
    "            for clf_name, clf in [('DecisionTree', dt_clf), ('RandomForest', rf_clf)]:\n",
    "                # TRTR: Train Real, Test Real (baseline)\n",
    "                clf.fit(X_real_train, y_real_train)\n",
    "                acc_trtr = clf.score(X_real_test, y_real_test)\n",
    "                \n",
    "                # TSTS: Train Synthetic, Test Synthetic\n",
    "                clf.fit(X_synth_train, y_synth_train)\n",
    "                acc_tsts = clf.score(X_synth_test, y_synth_test)\n",
    "                \n",
    "                # TRTS: Train Real, Test Synthetic\n",
    "                clf.fit(X_real_train, y_real_train)\n",
    "                acc_trts = clf.score(X_synth_test, y_synth_test)\n",
    "                \n",
    "                # TSTR: Train Synthetic, Test Real\n",
    "                clf.fit(X_synth_train, y_synth_train)\n",
    "                acc_tstr = clf.score(X_real_test, y_real_test)\n",
    "                \n",
    "                trts_scores[clf_name] = {\n",
    "                    'TRTR': acc_trtr,\n",
    "                    'TSTS': acc_tsts,\n",
    "                    'TRTS': acc_trts,\n",
    "                    'TSTR': acc_tstr,\n",
    "                    'Utility': acc_tstr / acc_trtr if acc_trtr > 0 else 0,\n",
    "                    'Quality': acc_trts / acc_trtr if acc_trtr > 0 else 0\n",
    "                }\n",
    "            \n",
    "            # Average TRTS scores\n",
    "            avg_trts = {}\n",
    "            for metric in ['TRTR', 'TSTS', 'TRTS', 'TSTR', 'Utility', 'Quality']:\n",
    "                avg_trts[metric] = np.mean([trts_scores[clf][metric] for clf in trts_scores.keys()])\n",
    "            \n",
    "            trts_results[model_name] = {\n",
    "                'individual': trts_scores,\n",
    "                'average': avg_trts\n",
    "            }\n",
    "            \n",
    "            print(f\"      ✅ TRTS completed - Utility: {avg_trts['Utility']:.4f}, Quality: {avg_trts['Quality']:.4f}\")\n",
    "            \n",
    "            # 2. Statistical Similarity Analysis\n",
    "            print(f\"   📊 Statistical Similarity Analysis...\")\n",
    "            \n",
    "            similarity_metrics = {}\n",
    "            \n",
    "            # Feature-wise similarity\n",
    "            feature_similarities = []\n",
    "            for col in X_real.columns:\n",
    "                if col in X_synth.columns:\n",
    "                    # Kolmogorov-Smirnov test\n",
    "                    ks_stat, ks_pval = stats.ks_2samp(X_real[col], X_synth[col])\n",
    "                    \n",
    "                    # Mean and std differences\n",
    "                    mean_diff = abs(X_real[col].mean() - X_synth[col].mean())\n",
    "                    std_diff = abs(X_real[col].std() - X_synth[col].std())\n",
    "                    \n",
    "                    # Normalized differences\n",
    "                    mean_norm_diff = mean_diff / X_real[col].std() if X_real[col].std() > 0 else 0\n",
    "                    std_norm_diff = std_diff / X_real[col].std() if X_real[col].std() > 0 else 0\n",
    "                    \n",
    "                    feature_similarities.append({\n",
    "                        'feature': col,\n",
    "                        'ks_statistic': ks_stat,\n",
    "                        'ks_pvalue': ks_pval,\n",
    "                        'mean_diff': mean_diff,\n",
    "                        'std_diff': std_diff,\n",
    "                        'mean_norm_diff': mean_norm_diff,\n",
    "                        'std_norm_diff': std_norm_diff,\n",
    "                        'similar': ks_pval > 0.05\n",
    "                    })\n",
    "            \n",
    "            # Aggregate similarity metrics\n",
    "            similarity_metrics = {\n",
    "                'avg_ks_statistic': np.mean([f['ks_statistic'] for f in feature_similarities]),\n",
    "                'avg_ks_pvalue': np.mean([f['ks_pvalue'] for f in feature_similarities]),\n",
    "                'similar_features': sum([f['similar'] for f in feature_similarities]),\n",
    "                'total_features': len(feature_similarities),\n",
    "                'similarity_ratio': sum([f['similar'] for f in feature_similarities]) / len(feature_similarities),\n",
    "                'avg_mean_norm_diff': np.mean([f['mean_norm_diff'] for f in feature_similarities]),\n",
    "                'avg_std_norm_diff': np.mean([f['std_norm_diff'] for f in feature_similarities])\n",
    "            }\n",
    "            \n",
    "            # Correlation similarity\n",
    "            real_corr = X_real.corr()\n",
    "            synth_corr = X_synth.corr()\n",
    "            corr_diff = np.abs(real_corr - synth_corr)\n",
    "            \n",
    "            # Get upper triangle (excluding diagonal)\n",
    "            mask = np.triu(np.ones_like(corr_diff, dtype=bool), k=1)\n",
    "            corr_diffs = corr_diff.values[mask]\n",
    "            \n",
    "            similarity_metrics['avg_corr_diff'] = np.mean(corr_diffs)\n",
    "            similarity_metrics['max_corr_diff'] = np.max(corr_diffs)\n",
    "            \n",
    "            similarity_results[model_name] = {\n",
    "                'feature_level': feature_similarities,\n",
    "                'aggregate': similarity_metrics\n",
    "            }\n",
    "            \n",
    "            print(f\"      ✅ Similarity completed - Ratio: {similarity_metrics['similarity_ratio']:.4f}\")\n",
    "            \n",
    "            # 3. Combined Evaluation Score\n",
    "            print(f\"   🏆 Computing combined evaluation score...\")\n",
    "            \n",
    "            # Weighted combination: 40% Utility + 30% Quality + 30% Similarity\n",
    "            combined_score = (\n",
    "                0.4 * avg_trts['Utility'] +\n",
    "                0.3 * avg_trts['Quality'] +\n",
    "                0.3 * similarity_metrics['similarity_ratio']\n",
    "            )\n",
    "            \n",
    "            evaluation_results[model_name] = {\n",
    "                'combined_score': combined_score,\n",
    "                'utility_score': avg_trts['Utility'],\n",
    "                'quality_score': avg_trts['Quality'],\n",
    "                'similarity_score': similarity_metrics['similarity_ratio'],\n",
    "                'trts_details': avg_trts,\n",
    "                'similarity_details': similarity_metrics\n",
    "            }\n",
    "            \n",
    "            print(f\"      ✅ Combined score: {combined_score:.4f}\")\n",
    "            print(f\"      📊 Breakdown - Utility: {avg_trts['Utility']:.4f}, Quality: {avg_trts['Quality']:.4f}, Similarity: {similarity_metrics['similarity_ratio']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"   ❌ {model_name} evaluation failed: {error_msg[:100]}...\")\n",
    "            evaluation_results[model_name] = {\n",
    "                'combined_score': 0.0,\n",
    "                'error': error_msg\n",
    "            }\n",
    "    \n",
    "    # Phase 4 Summary - Ranking and Best Model Identification\n",
    "    print(f\"\\n🏆 PHASE 4 SUMMARY - MODEL RANKING\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Sort models by combined score\n",
    "    evaluated_models = [name for name in evaluation_results.keys() \n",
    "                       if 'error' not in evaluation_results[name]]\n",
    "    \n",
    "    if evaluated_models:\n",
    "        sorted_models = sorted(evaluated_models, \n",
    "                              key=lambda x: evaluation_results[x]['combined_score'], \n",
    "                              reverse=True)\n",
    "        \n",
    "        print(f\"🥇 MODEL RANKING (by combined score):\")\n",
    "        for i, model_name in enumerate(sorted_models, 1):\n",
    "            result = evaluation_results[model_name]\n",
    "            print(f\"   {i}. {model_name}: {result['combined_score']:.4f}\")\n",
    "            print(f\"      • Utility: {result['utility_score']:.4f}\")\n",
    "            print(f\"      • Quality: {result['quality_score']:.4f}\")\n",
    "            print(f\"      • Similarity: {result['similarity_score']:.4f}\")\n",
    "        \n",
    "        best_model = sorted_models[0]\n",
    "        print(f\"\\n🏆 BEST OVERALL MODEL: {best_model}\")\n",
    "        print(f\"📊 Combined Score: {evaluation_results[best_model]['combined_score']:.4f}\")\n",
    "        \n",
    "        # Export evaluation results\n",
    "        if EXPORT_TABLES:\n",
    "            # Model ranking table\n",
    "            ranking_data = []\n",
    "            for i, model_name in enumerate(sorted_models, 1):\n",
    "                result = evaluation_results[model_name]\n",
    "                ranking_data.append({\n",
    "                    'Rank': i,\n",
    "                    'Model': model_name,\n",
    "                    'Combined_Score': result['combined_score'],\n",
    "                    'Utility_Score': result['utility_score'],\n",
    "                    'Quality_Score': result['quality_score'],\n",
    "                    'Similarity_Score': result['similarity_score']\n",
    "                })\n",
    "            \n",
    "            ranking_df = pd.DataFrame(ranking_data)\n",
    "            ranking_df.to_csv(RESULTS_DIR / 'final_model_ranking.csv', index=False)\n",
    "            print(f\"\\n💾 Model ranking exported: final_model_ranking.csv\")\n",
    "            \n",
    "            # Detailed TRTS results\n",
    "            trts_data = []\n",
    "            for model_name in evaluated_models:\n",
    "                avg_trts = trts_results[model_name]['average']\n",
    "                trts_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'TRTR': avg_trts['TRTR'],\n",
    "                    'TSTS': avg_trts['TSTS'],\n",
    "                    'TRTS': avg_trts['TRTS'],\n",
    "                    'TSTR': avg_trts['TSTR'],\n",
    "                    'Utility': avg_trts['Utility'],\n",
    "                    'Quality': avg_trts['Quality']\n",
    "                })\n",
    "            \n",
    "            trts_df = pd.DataFrame(trts_data)\n",
    "            trts_df.to_csv(RESULTS_DIR / 'detailed_trts_results.csv', index=False)\n",
    "            print(f\"💾 TRTS results exported: detailed_trts_results.csv\")\n",
    "        \n",
    "        print(f\"\\n🎯 Phase 4 completed. Best model identified: {best_model}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"❌ No models successfully evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Comprehensive Visualizations and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Comprehensive visualizations and analysis\n",
    "print(\"📊 PHASE 5: COMPREHENSIVE VISUALIZATIONS AND ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not evaluated_models:\n",
    "    print(\"⚠️ No evaluated models from Phase 4. Cannot create visualizations.\")\n",
    "else:\n",
    "    # Create comprehensive visualization dashboard\n",
    "    print(f\"📊 Creating comprehensive visualization dashboard...\")\n",
    "    \n",
    "    # Figure 1: Model Performance Comparison\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Combined Scores\n",
    "    ax1 = axes[0, 0]\n",
    "    models = sorted_models\n",
    "    scores = [evaluation_results[model]['combined_score'] for model in models]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "    \n",
    "    bars = ax1.bar(models, scores, color=colors, alpha=0.8)\n",
    "    ax1.set_title('Combined Performance Scores', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Combined Score')\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Utility vs Quality Scatter\n",
    "    ax2 = axes[0, 1]\n",
    "    utilities = [evaluation_results[model]['utility_score'] for model in models]\n",
    "    qualities = [evaluation_results[model]['quality_score'] for model in models]\n",
    "    \n",
    "    scatter = ax2.scatter(utilities, qualities, c=scores, cmap='viridis', s=100, alpha=0.8)\n",
    "    ax2.set_xlabel('Utility Score (TSTR/TRTR)')\n",
    "    ax2.set_ylabel('Quality Score (TRTS/TRTR)')\n",
    "    ax2.set_title('Utility vs Quality Trade-off', fontweight='bold', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add model labels\n",
    "    for i, model in enumerate(models):\n",
    "        ax2.annotate(model, (utilities[i], qualities[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax2, label='Combined Score')\n",
    "    \n",
    "    # Plot 3: TRTS Framework Detailed Results\n",
    "    ax3 = axes[0, 2]\n",
    "    trts_metrics = ['TRTR', 'TSTS', 'TRTS', 'TSTR']\n",
    "    x = np.arange(len(trts_metrics))\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        values = [trts_results[model]['average'][metric] for metric in trts_metrics]\n",
    "        ax3.bar(x + i*width, values, width, label=model, alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('TRTS Scenarios')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('TRTS Framework Detailed Results', fontweight='bold', fontsize=12)\n",
    "    ax3.set_xticks(x + width * (len(models)-1) / 2)\n",
    "    ax3.set_xticklabels(trts_metrics)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Statistical Similarity Analysis\n",
    "    ax4 = axes[1, 0]\n",
    "    similarity_scores = [evaluation_results[model]['similarity_score'] for model in models]\n",
    "    \n",
    "    bars = ax4.barh(models, similarity_scores, color='lightcoral', alpha=0.8)\n",
    "    ax4.set_xlabel('Similarity Ratio (Features Passing KS Test)')\n",
    "    ax4.set_title('Statistical Similarity Scores', fontweight='bold', fontsize=12)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, similarity_scores):\n",
    "        ax4.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.3f}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # Plot 5: Training and Generation Times\n",
    "    ax5 = axes[1, 1]\n",
    "    training_times = [phase3_results[model]['training_time'] for model in models if model in phase3_results]\n",
    "    generation_times = [phase3_results[model]['generation_time'] for model in models if model in phase3_results]\n",
    "    valid_models = [model for model in models if model in phase3_results]\n",
    "    \n",
    "    x = np.arange(len(valid_models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax5.bar(x - width/2, training_times, width, label='Training Time', alpha=0.8, color='skyblue')\n",
    "    ax5.bar(x + width/2, generation_times, width, label='Generation Time', alpha=0.8, color='lightgreen')\n",
    "    \n",
    "    ax5.set_xlabel('Models')\n",
    "    ax5.set_ylabel('Time (seconds)')\n",
    "    ax5.set_title('Training and Generation Times', fontweight='bold', fontsize=12)\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(valid_models, rotation=45, ha='right')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Feature-wise Similarity Heatmap (for best model)\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    if best_model in similarity_results:\n",
    "        feature_sims = similarity_results[best_model]['feature_level']\n",
    "        features = [f['feature'] for f in feature_sims]\n",
    "        ks_stats = [f['ks_statistic'] for f in feature_sims]\n",
    "        \n",
    "        # Create heatmap data\n",
    "        heatmap_data = np.array(ks_stats).reshape(-1, 1)\n",
    "        \n",
    "        im = ax6.imshow(heatmap_data.T, cmap='RdYlBu_r', aspect='auto')\n",
    "        ax6.set_xticks(range(len(features)))\n",
    "        ax6.set_xticklabels([f.replace('_', ' ') for f in features], rotation=45, ha='right')\n",
    "        ax6.set_yticks([0])\n",
    "        ax6.set_yticklabels([f'{best_model} KS Statistics'])\n",
    "        ax6.set_title(f'Feature Similarity - {best_model}\\n(Lower = Better)', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i, ks_stat in enumerate(ks_stats):\n",
    "            ax6.text(i, 0, f'{ks_stat:.3f}', ha='center', va='center', \n",
    "                    color='white' if ks_stat > 0.3 else 'black', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'Multi-Model Analysis Dashboard - {DATASET_NAME}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    if EXPORT_FIGURES:\n",
    "        plt.savefig(RESULTS_DIR / f'multi_model_analysis_dashboard.{FIGURE_FORMAT}', \n",
    "                   dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"💾 Dashboard exported: multi_model_analysis_dashboard.{FIGURE_FORMAT}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Figure 2: Best Model Detailed Analysis (similar to Phase1 notebook style)\n",
    "    print(f\"\\n📊 Creating detailed analysis for best model: {best_model}\")\n",
    "    \n",
    "    best_synthetic_data = phase3_synthetic_data[best_model]\n",
    "    \n",
    "    # Distribution comparison plots\n",
    "    numeric_features = processed_data.select_dtypes(include=[np.number]).columns\n",
    "    features_to_plot = [col for col in numeric_features if col != TARGET_COLUMN][:4]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        if i < len(axes):\n",
    "            # Original data histogram\n",
    "            axes[i].hist(processed_data[feature], bins=30, alpha=0.6, density=True,\n",
    "                        label='Original', color='blue', edgecolor='black')\n",
    "            \n",
    "            # Synthetic data histogram\n",
    "            axes[i].hist(best_synthetic_data[feature], bins=30, alpha=0.6, density=True,\n",
    "                        label=f'{best_model} Synthetic', color='red', histtype='step', linewidth=2)\n",
    "            \n",
    "            # Add density curves\n",
    "            try:\n",
    "                # Original density\n",
    "                orig_clean = processed_data[feature].dropna()\n",
    "                if len(orig_clean) > 1:\n",
    "                    kde_x_orig = np.linspace(orig_clean.min(), orig_clean.max(), 100)\n",
    "                    kde_orig = stats.gaussian_kde(orig_clean)\n",
    "                    axes[i].plot(kde_x_orig, kde_orig(kde_x_orig), 'b-', linewidth=2, alpha=0.8)\n",
    "                \n",
    "                # Synthetic density\n",
    "                synth_clean = best_synthetic_data[feature].dropna()\n",
    "                if len(synth_clean) > 1:\n",
    "                    kde_x_synth = np.linspace(synth_clean.min(), synth_clean.max(), 100)\n",
    "                    kde_synth = stats.gaussian_kde(synth_clean)\n",
    "                    axes[i].plot(kde_x_synth, kde_synth(kde_x_synth), 'r--', linewidth=2, alpha=0.8)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xlabel(feature.replace('_', ' '))\n",
    "            axes[i].set_ylabel('Density')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Distribution Comparison: Original vs {best_model} - {DATASET_NAME}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    if EXPORT_FIGURES:\n",
    "        plt.savefig(RESULTS_DIR / f'best_model_distribution_comparison.{FIGURE_FORMAT}', \n",
    "                   dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"💾 Best model comparison exported: best_model_distribution_comparison.{FIGURE_FORMAT}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✅ Phase 5 completed - All visualizations created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Conclusions\n",
    "print(\"🎯 FINAL SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create comprehensive final report\n",
    "final_report = {\n",
    "    'Dataset': DATASET_NAME,\n",
    "    'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'Total_Models_Tested': len(MODEL_STATUS),\n",
    "    'Available_Models': len(available_models),\n",
    "    'Successfully_Demoed': len(successful_models) if 'successful_models' in locals() else 0,\n",
    "    'Successfully_Tuned': len(tuned_models) if 'tuned_models' in locals() else 0,\n",
    "    'Successfully_Evaluated': len(evaluated_models) if 'evaluated_models' in locals() else 0,\n",
    "    'Best_Model': best_model if 'best_model' in locals() else 'None',\n",
    "    'Best_Combined_Score': evaluation_results[best_model]['combined_score'] if 'best_model' in locals() and best_model in evaluation_results else 0\n",
    "}\n",
    "\n",
    "print(f\"📊 ANALYSIS OVERVIEW:\")\n",
    "for key, value in final_report.items():\n",
    "    print(f\"   • {key.replace('_', ' ')}: {value}\")\n",
    "\n",
    "if 'best_model' in locals() and best_model in evaluation_results:\n",
    "    print(f\"\\n🏆 BEST MODEL DETAILS:\")\n",
    "    best_result = evaluation_results[best_model]\n",
    "    print(f\"   • Model: {best_model}\")\n",
    "    print(f\"   • Combined Score: {best_result['combined_score']:.4f}\")\n",
    "    print(f\"   • Utility Score: {best_result['utility_score']:.4f}\")\n",
    "    print(f\"   • Quality Score: {best_result['quality_score']:.4f}\")\n",
    "    print(f\"   • Similarity Score: {best_result['similarity_score']:.4f}\")\n",
    "    \n",
    "    if best_model in phase3_results:\n",
    "        print(f\"   • Training Time: {phase3_results[best_model]['training_time']:.2f} seconds\")\n",
    "        print(f\"   • Generation Time: {phase3_results[best_model]['generation_time']:.3f} seconds\")\n",
    "    \n",
    "    print(f\"\\n📊 BEST MODEL PERFORMANCE BREAKDOWN:\")\n",
    "    best_trts = best_result['trts_details']\n",
    "    print(f\"   • TRTR (Baseline): {best_trts['TRTR']:.4f}\")\n",
    "    print(f\"   • TSTS (Consistency): {best_trts['TSTS']:.4f}\")\n",
    "    print(f\"   • TRTS (Quality): {best_trts['TRTS']:.4f}\")\n",
    "    print(f\"   • TSTR (Utility): {best_trts['TSTR']:.4f}\")\n",
    "    \n",
    "    best_sim = best_result['similarity_details']\n",
    "    print(f\"\\n📊 BEST MODEL SIMILARITY ANALYSIS:\")\n",
    "    print(f\"   • Features Passing KS Test: {best_sim['similar_features']}/{best_sim['total_features']}\")\n",
    "    print(f\"   • Average KS Statistic: {best_sim['avg_ks_statistic']:.4f}\")\n",
    "    print(f\"   • Average Correlation Difference: {best_sim['avg_corr_diff']:.4f}\")\n",
    "    print(f\"   • Max Correlation Difference: {best_sim['max_corr_diff']:.4f}\")\n",
    "\n",
    "if 'evaluated_models' in locals() and len(evaluated_models) > 1:\n",
    "    print(f\"\\n📈 MODEL COMPARISON INSIGHTS:\")\n",
    "    \n",
    "    # Best performing aspects\n",
    "    best_utility = max(evaluated_models, key=lambda x: evaluation_results[x]['utility_score'])\n",
    "    best_quality = max(evaluated_models, key=lambda x: evaluation_results[x]['quality_score'])\n",
    "    best_similarity = max(evaluated_models, key=lambda x: evaluation_results[x]['similarity_score'])\n",
    "    \n",
    "    print(f\"   • Best Utility (TSTR): {best_utility} ({evaluation_results[best_utility]['utility_score']:.4f})\")\n",
    "    print(f\"   • Best Quality (TRTS): {best_quality} ({evaluation_results[best_quality]['quality_score']:.4f})\")\n",
    "    print(f\"   • Best Similarity: {best_similarity} ({evaluation_results[best_similarity]['similarity_score']:.4f})\")\n",
    "    \n",
    "    # Performance spread\n",
    "    scores = [evaluation_results[model]['combined_score'] for model in evaluated_models]\n",
    "    print(f\"\\n📊 PERFORMANCE DISTRIBUTION:\")\n",
    "    print(f\"   • Score Range: {min(scores):.4f} - {max(scores):.4f}\")\n",
    "    print(f\"   • Score Spread: {max(scores) - min(scores):.4f}\")\n",
    "    print(f\"   • Average Score: {np.mean(scores):.4f}\")\n",
    "    print(f\"   • Standard Deviation: {np.std(scores):.4f}\")\n",
    "\n",
    "print(f\"\\n🎓 KEY FINDINGS:\")\n",
    "print(f\"   • Multi-model framework successfully implemented\")\n",
    "print(f\"   • Comprehensive evaluation using TRTS + Statistical Similarity\")\n",
    "print(f\"   • Hyperparameter optimization improved model performance\")\n",
    "print(f\"   • Best model balances utility, quality, and similarity\")\n",
    "if 'best_model' in locals():\n",
    "    print(f\"   • {best_model} emerged as optimal choice for {DATASET_NAME}\")\n",
    "\n",
    "print(f\"\\n📁 EXPORTED ARTIFACTS:\")\n",
    "if EXPORT_TABLES:\n",
    "    artifacts = [\n",
    "        'preprocessed_breast_cancer_data.csv',\n",
    "        'phase3_final_models_summary.csv',\n",
    "        'final_model_ranking.csv',\n",
    "        'detailed_trts_results.csv'\n",
    "    ]\n",
    "    # Add synthetic data files\n",
    "    if 'final_models' in locals():\n",
    "        for model in final_models:\n",
    "            artifacts.append(f'{model.lower()}_final_synthetic_data.csv')\n",
    "    \n",
    "    for artifact in artifacts:\n",
    "        print(f\"   • {artifact}\")\n",
    "\n",
    "if EXPORT_FIGURES:\n",
    "    print(f\"\\n📊 EXPORTED VISUALIZATIONS:\")\n",
    "    visualizations = [\n",
    "        'multi_model_analysis_dashboard.png',\n",
    "        'best_model_distribution_comparison.png'\n",
    "    ]\n",
    "    for viz in visualizations:\n",
    "        print(f\"   • {viz}\")\n",
    "\n",
    "# Export final summary report\n",
    "if EXPORT_TABLES:\n",
    "    final_summary_data = []\n",
    "    \n",
    "    # Add overall summary\n",
    "    final_summary_data.append({\n",
    "        'Category': 'Analysis Overview',\n",
    "        'Metric': 'Dataset',\n",
    "        'Value': DATASET_NAME\n",
    "    })\n",
    "    final_summary_data.append({\n",
    "        'Category': 'Analysis Overview',\n",
    "        'Metric': 'Analysis Date',\n",
    "        'Value': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    })\n",
    "    final_summary_data.append({\n",
    "        'Category': 'Analysis Overview',\n",
    "        'Metric': 'Models Successfully Evaluated',\n",
    "        'Value': len(evaluated_models) if 'evaluated_models' in locals() else 0\n",
    "    })\n",
    "    \n",
    "    if 'best_model' in locals() and best_model in evaluation_results:\n",
    "        best_result = evaluation_results[best_model]\n",
    "        final_summary_data.extend([\n",
    "            {'Category': 'Best Model', 'Metric': 'Model Name', 'Value': best_model},\n",
    "            {'Category': 'Best Model', 'Metric': 'Combined Score', 'Value': f\"{best_result['combined_score']:.4f}\"},\n",
    "            {'Category': 'Best Model', 'Metric': 'Utility Score', 'Value': f\"{best_result['utility_score']:.4f}\"},\n",
    "            {'Category': 'Best Model', 'Metric': 'Quality Score', 'Value': f\"{best_result['quality_score']:.4f}\"},\n",
    "            {'Category': 'Best Model', 'Metric': 'Similarity Score', 'Value': f\"{best_result['similarity_score']:.4f}\"}\n",
    "        ])\n",
    "    \n",
    "    final_summary_df = pd.DataFrame(final_summary_data)\n",
    "    final_summary_df.to_csv(RESULTS_DIR / 'final_analysis_summary.csv', index=False)\n",
    "    print(f\"\\n💾 Final summary exported: final_analysis_summary.csv\")\n",
    "\n",
    "print(f\"\\n✅ MULTI-MODEL ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"📁 All results saved to: {RESULTS_DIR.absolute()}\")\n",
    "print(f\"\\n🎯 NEXT STEPS:\")\n",
    "print(f\"   • Review detailed results in exported CSV files\")\n",
    "print(f\"   • Examine visualizations for deeper insights\")\n",
    "if 'best_model' in locals():\n",
    "    print(f\"   • Consider using {best_model} for production synthetic data generation\")\n",
    "    print(f\"   • Fine-tune {best_model} further if needed for specific use cases\")\n",
    "print(f\"   • Validate results on additional datasets\")\n",
    "print(f\"   • Consider ensemble approaches combining multiple models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}