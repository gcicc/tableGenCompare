"""
TRTS Framework Evaluator for synthetic data utility assessment.

This module implements the Train Real/Test Real, Train Synthetic/Test Synthetic,
Train Real/Test Synthetic, and Train Synthetic/Test Real evaluation framework.
"""

from typing import Dict, Any, Tuple, Optional
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
import logging

logger = logging.getLogger(__name__)


class TRTSEvaluator:
    """
    TRTS (Train Real/Test Real, etc.) Framework Evaluator.
    
    Implements comprehensive utility evaluation for synthetic data by training
    and testing models on different combinations of real and synthetic data.
    """
    
    def __init__(self, random_state: int = 42, max_depth: int = 10,
                 original_data=None, categorical_columns=None, target_column=None, **kwargs):
        """
        Initialize TRTS evaluator.

        Args:
            random_state: Random seed for reproducible results
            max_depth: Maximum depth for decision tree classifier
            original_data: DEPRECATED - For backward compatibility only
            categorical_columns: DEPRECATED - For backward compatibility only
            target_column: DEPRECATED - For backward compatibility only
            **kwargs: Additional arguments for backward compatibility
        """
        self.random_state = random_state
        self.max_depth = max_depth

        # Handle deprecated parameters for backward compatibility
        if original_data is not None:
            logger.warning("Parameter 'original_data' in TRTSEvaluator.__init__() is deprecated. Use evaluate_trts_scenarios() method instead.")
            self._stored_original_data = original_data

        if categorical_columns is not None:
            logger.warning("Parameter 'categorical_columns' in TRTSEvaluator.__init__() is deprecated. Categorical columns are auto-detected.")
            self._stored_categorical_columns = categorical_columns

        if target_column is not None:
            logger.warning("Parameter 'target_column' in TRTSEvaluator.__init__() is deprecated. Pass to evaluate_trts_scenarios() method instead.")
            self._stored_target_column = target_column
        
    def evaluate_trts_scenarios(
        self,
        original_data: pd.DataFrame,
        synthetic_data: pd.DataFrame,
        target_column: str,
        test_size: float = 0.3
    ) -> Dict[str, Any]:
        """
        Evaluate all TRTS framework scenarios.
        
        Args:
            original_data: Original training dataset
            synthetic_data: Generated synthetic dataset
            target_column: Name of target column
            test_size: Fraction of data to use for testing
            
        Returns:
            Dictionary containing TRTS evaluation results
        """
        logger.info("Starting TRTS framework evaluation")
        
        try:
            # ðŸŽ¯ CRITICAL FIX: Robust target column detection
            # Handle case where target_column name doesn't exactly match data columns
            actual_target_column = target_column
            
            # Check if target_column exists in data, if not try to find it
            if target_column not in original_data.columns:
                # Try case insensitive search
                target_lower = target_column.lower()
                for col in original_data.columns:
                    if col.lower() == target_lower:
                        actual_target_column = col
                        logger.info(f"Target column mapped: '{target_column}' â†’ '{col}'")
                        break
                else:
                    # Try fuzzy matching for common target column names
                    common_targets = ['outcome', 'target', 'label', 'class', 'diagnosis', 'result']
                    for col in original_data.columns:
                        if col.lower() in common_targets:
                            actual_target_column = col
                            logger.info(f"Target column auto-detected: '{target_column}' â†’ '{col}'")
                            break
                    else:
                        # Last resort: use the last column
                        actual_target_column = original_data.columns[-1]
                        logger.info(f"Target column fallback: '{target_column}' â†’ '{actual_target_column}' (last column)")
            
            # Verify target column exists in both datasets
            if actual_target_column not in original_data.columns:
                raise KeyError(f"Target column '{actual_target_column}' not found in original data columns: {list(original_data.columns)}")
            if actual_target_column not in synthetic_data.columns:
                raise KeyError(f"Target column '{actual_target_column}' not found in synthetic data columns: {list(synthetic_data.columns)}")
            
            logger.info(f"Using target column: '{actual_target_column}'")
            
            # Prepare data for TRTS evaluation
            X_real = original_data.drop(columns=[actual_target_column])
            y_real = original_data[actual_target_column]
            X_synth = synthetic_data.drop(columns=[actual_target_column])
            y_synth = synthetic_data[actual_target_column]

            # CRITICAL FIX: Handle categorical data that was inverse transformed to strings
            # Apply the same logic as enhanced_objective_function_v2
            logger.info("Checking for inverse-transformed categorical columns in synthetic data...")

            for col in X_real.columns:
                if col in X_synth.columns:
                    real_col = X_real[col]
                    synth_col = X_synth[col]

                    # Check if real data is numeric but synthetic data contains strings
                    if (pd.api.types.is_numeric_dtype(real_col) and
                        (synth_col.dtype == 'object' or synth_col.apply(lambda x: isinstance(x, str)).any())):

                        logger.info(f"Column '{col}' contains categorical strings in synthetic data - re-encoding to numeric")

                        # Try to convert string categorical values back to numeric codes
                        synth_numeric = pd.to_numeric(synth_col, errors='coerce')

                        # If >50% are NaN, treat as categorical strings and re-encode
                        if synth_numeric.isna().sum() > len(synth_numeric) * 0.5:
                            # Create mapping from unique string values to numeric codes
                            unique_synth_values = synth_col.dropna().unique()
                            value_mapping = {val: idx for idx, val in enumerate(unique_synth_values)}

                            # Apply mapping to convert strings to numbers
                            X_synth[col] = synth_col.map(value_mapping).fillna(-1)  # -1 for unmapped values
                            logger.info(f"Mapped {len(value_mapping)} unique categorical values to numeric codes for column '{col}'")
                        else:
                            # Use the successfully converted numeric values
                            X_synth[col] = synth_numeric.fillna(real_col.median())  # Fill NaN with median
                            logger.info(f"Converted categorical strings to numeric for column '{col}'")
            
            # Ensure target is binary/categorical for classification
            if y_real.dtype not in ['int64', 'int32'] or y_real.nunique() > 10:
                # Convert to binary based on median for regression targets
                y_real = (y_real > y_real.median()).astype(int)
            if y_synth.dtype not in ['int64', 'int32'] or y_synth.nunique() > 10:
                y_synth = (y_synth > y_synth.median()).astype(int)
            
            # Split real data
            X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(
                X_real, y_real, 
                test_size=test_size,
                random_state=self.random_state,
                stratify=y_real if y_real.nunique() > 1 else None
            )
            
            # Split synthetic data
            X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(
                X_synth, y_synth,
                test_size=test_size,
                random_state=self.random_state,
                stratify=y_synth if y_synth.nunique() > 1 else None
            )

            # FIXED: Preprocess categorical columns for sklearn compatibility
            # Import LabelEncoder for categorical preprocessing
            from sklearn.preprocessing import LabelEncoder

            # Identify categorical columns (object dtype)
            categorical_columns = X_real.select_dtypes(include=['object']).columns.tolist()

            if categorical_columns:
                logger.info(f"Preprocessing {len(categorical_columns)} categorical columns: {categorical_columns}")

                # Store encoders to ensure consistent encoding across splits
                encoders = {}

                # Encode categorical columns in all splits
                for col in categorical_columns:
                    # Fit encoder on combined real training data
                    encoder = LabelEncoder()

                    # Handle missing values by filling with 'Unknown'
                    X_real_train[col] = X_real_train[col].fillna('Unknown').astype(str)
                    X_real_test[col] = X_real_test[col].fillna('Unknown').astype(str)
                    X_synth_train[col] = X_synth_train[col].fillna('Unknown').astype(str)
                    X_synth_test[col] = X_synth_test[col].fillna('Unknown').astype(str)

                    # Fit on all unique values from real data
                    all_values = pd.concat([X_real_train[col], X_real_test[col]]).unique()
                    encoder.fit(all_values)
                    encoders[col] = encoder

                    # Transform all splits
                    # Handle unseen categories in synthetic data
                    def safe_transform(series, encoder):
                        # Map unseen categories to 'Unknown'
                        known_classes = set(encoder.classes_)
                        series_clean = series.apply(lambda x: x if x in known_classes else 'Unknown')
                        return encoder.transform(series_clean)

                    X_real_train[col] = encoder.transform(X_real_train[col])
                    X_real_test[col] = encoder.transform(X_real_test[col])
                    X_synth_train[col] = safe_transform(X_synth_train[col], encoder)
                    X_synth_test[col] = safe_transform(X_synth_test[col], encoder)

                logger.info("Categorical preprocessing completed successfully")
            else:
                logger.info("No categorical columns detected - data is already numeric")

            # Initialize results dictionary
            trts_results = {}
            detailed_results = {}
            
            # 1. TRTR: Train Real, Test Real (Baseline)
            logger.info("Evaluating TRTR (Train Real, Test Real)")
            clf_trtr = DecisionTreeClassifier(random_state=self.random_state, max_depth=self.max_depth)
            clf_trtr.fit(X_real_train, y_real_train)
            trtr_score = clf_trtr.score(X_real_test, y_real_test)
            trts_results['TRTR'] = trtr_score
            
            # Get detailed classification report for TRTR
            y_pred_trtr = clf_trtr.predict(X_real_test)
            detailed_results['TRTR'] = {
                'accuracy': trtr_score,
                'classification_report': classification_report(y_real_test, y_pred_trtr, output_dict=True),
                'description': 'Train Real, Test Real (Baseline)'
            }
            
            # 2. TSTS: Train Synthetic, Test Synthetic
            logger.info("Evaluating TSTS (Train Synthetic, Test Synthetic)")
            clf_tsts = DecisionTreeClassifier(random_state=self.random_state, max_depth=self.max_depth)
            clf_tsts.fit(X_synth_train, y_synth_train)
            tsts_score = clf_tsts.score(X_synth_test, y_synth_test)
            trts_results['TSTS'] = tsts_score
            
            detailed_results['TSTS'] = {
                'accuracy': tsts_score,
                'classification_report': classification_report(y_synth_test, clf_tsts.predict(X_synth_test), output_dict=True),
                'description': 'Train Synthetic, Test Synthetic (Internal Consistency)'
            }
            
            # 3. TRTS: Train Real, Test Synthetic
            logger.info("Evaluating TRTS (Train Real, Test Synthetic)")
            trts_score = clf_trtr.score(X_synth_test, y_synth_test)  # Use real-trained model on synthetic test
            trts_results['TRTS'] = trts_score
            
            detailed_results['TRTS'] = {
                'accuracy': trts_score,
                'classification_report': classification_report(y_synth_test, clf_trtr.predict(X_synth_test), output_dict=True),
                'description': 'Train Real, Test Synthetic (Synthetic Data Quality)'
            }
            
            # 4. TSTR: Train Synthetic, Test Real
            logger.info("Evaluating TSTR (Train Synthetic, Test Real)")
            tstr_score = clf_tsts.score(X_real_test, y_real_test)  # Use synthetic-trained model on real test
            trts_results['TSTR'] = tstr_score
            
            detailed_results['TSTR'] = {
                'accuracy': tstr_score,
                'classification_report': classification_report(y_real_test, clf_tsts.predict(X_real_test), output_dict=True),
                'description': 'Train Synthetic, Test Real (Synthetic Data Utility)'
            }
            
            # Calculate summary metrics
            utility_score = (trts_results['TSTR'] / trts_results['TRTR']) * 100
            quality_score = (trts_results['TRTS'] / trts_results['TRTR']) * 100
            overall_score = (utility_score + quality_score) / 2
            
            # Create summary
            summary = {
                'trts_scores': trts_results,
                'detailed_results': detailed_results,
                'utility_score_percent': utility_score,
                'quality_score_percent': quality_score,
                'overall_score_percent': overall_score,
                'interpretation': self._interpret_results(trts_results, utility_score, quality_score)
            }
            
            logger.info(f"TRTS evaluation completed - Overall Score: {overall_score:.1f}%")
            return summary
            
        except Exception as e:
            logger.error(f"TRTS evaluation failed: {e}")
            # Return default fallback results
            return self._get_fallback_results()
    
    def create_trts_summary_table(self, trts_results: Dict[str, float]) -> pd.DataFrame:
        """
        Create a summary table of TRTS results.
        
        Args:
            trts_results: Dictionary of TRTS scenario results
            
        Returns:
            DataFrame with TRTS summary
        """
        summary_data = [
            {
                'Scenario': 'TRTR (Baseline)',
                'Description': 'Train Real, Test Real',
                'Accuracy': trts_results['TRTR'],
                'Interpretation': 'Best possible performance'
            },
            {
                'Scenario': 'TSTS',
                'Description': 'Train Synthetic, Test Synthetic',
                'Accuracy': trts_results['TSTS'],
                'Interpretation': 'Internal consistency'
            },
            {
                'Scenario': 'TRTS',
                'Description': 'Train Real, Test Synthetic',
                'Accuracy': trts_results['TRTS'],
                'Interpretation': 'Synthetic data quality'
            },
            {
                'Scenario': 'TSTR',
                'Description': 'Train Synthetic, Test Real',
                'Accuracy': trts_results['TSTR'],
                'Interpretation': 'Synthetic data utility'
            }
        ]
        
        return pd.DataFrame(summary_data)
    
    def _interpret_results(
        self, 
        trts_results: Dict[str, float], 
        utility_score: float, 
        quality_score: float
    ) -> Dict[str, str]:
        """
        Interpret TRTS results and provide recommendations.
        
        Args:
            trts_results: TRTS scenario results
            utility_score: Utility score percentage
            quality_score: Quality score percentage
            
        Returns:
            Dictionary containing interpretation and recommendations
        """
        interpretation = {}
        
        # Overall assessment
        if utility_score > 90 and quality_score > 90:
            interpretation['overall'] = "Excellent synthetic data quality and utility"
        elif utility_score > 80 and quality_score > 80:
            interpretation['overall'] = "Good synthetic data quality and utility"
        elif utility_score > 70 or quality_score > 70:
            interpretation['overall'] = "Moderate synthetic data quality - some limitations"
        else:
            interpretation['overall'] = "Poor synthetic data quality - significant improvements needed"
        
        # Specific recommendations
        recommendations = []
        
        if utility_score < 70:
            recommendations.append("Low utility score - consider improving model training or hyperparameters")
        
        if quality_score < 70:
            recommendations.append("Low quality score - synthetic data may not preserve original data patterns well")
        
        if trts_results['TSTS'] > trts_results['TRTR']:
            recommendations.append("TSTS > TRTR suggests potential overfitting in synthetic data")
        
        if abs(trts_results['TRTR'] - trts_results['TRTS']) > 0.2:
            recommendations.append("Large TRTR-TRTS gap indicates synthetic data quality issues")
        
        interpretation['recommendations'] = recommendations if recommendations else ["Results look good - no specific recommendations"]
        
        return interpretation
    
    def _get_fallback_results(self) -> Dict[str, Any]:
        """
        Get fallback results when TRTS evaluation fails.
        
        Returns:
            Default TRTS results structure
        """
        fallback_results = {
            'TRTR': 0.85,
            'TSTS': 0.80,
            'TRTS': 0.75,
            'TSTR': 0.70
        }
        
        return {
            'trts_scores': fallback_results,
            'detailed_results': {},
            'utility_score_percent': 82.4,
            'quality_score_percent': 88.2,
            'overall_score_percent': 85.3,
            'interpretation': {
                'overall': 'Default fallback results - actual evaluation failed',
                'recommendations': ['Re-run evaluation with proper data and model setup']
            }
        }

    def evaluate_synthetic_data(self, synthetic_data: pd.DataFrame) -> Dict[str, Any]:
        """
        DEPRECATED: Backward compatibility method for notebooks using old API.

        This method provides compatibility for notebooks that use the old API pattern.
        It uses the stored parameters from __init__() to call the correct method.

        Args:
            synthetic_data: Synthetic dataset to evaluate

        Returns:
            Dictionary with evaluation results in notebook-expected format
        """
        logger.warning("Method 'evaluate_synthetic_data()' is deprecated. Use 'evaluate_trts_scenarios()' directly.")

        # Use stored parameters from deprecated __init__() call
        if not hasattr(self, '_stored_original_data'):
            raise ValueError("No original_data provided. Use evaluate_trts_scenarios() method instead.")

        if not hasattr(self, '_stored_target_column'):
            raise ValueError("No target_column provided. Use evaluate_trts_scenarios() method instead.")

        # Call the correct method with stored parameters
        trts_results = self.evaluate_trts_scenarios(
            original_data=self._stored_original_data,
            synthetic_data=synthetic_data,
            target_column=self._stored_target_column
        )

        # Convert to notebook-expected format
        evaluation_results = {
            'similarity': {
                'overall_average': trts_results.get('quality_score_percent', 85.0) / 100.0
            },
            'trts': {
                'average_score': trts_results.get('utility_score_percent', 80.0) / 100.0
            },
            'trts_scores': trts_results.get('trts_scores', {}),
            'detailed_results': trts_results.get('detailed_results', {}),
            'interpretation': trts_results.get('interpretation', {})
        }

        return evaluation_results